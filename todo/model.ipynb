{
 "metadata": {
  "name": "model"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Introduction:\n",
      "\n",
      "In order to grade physicians\u2019 performance on specific tasks a model of good and novice physicians must be created.  The specific steps to create this model are:  \n",
      "1.  Vector Quantization: reducing the total number of dimensions and normalizing the data  \n",
      "2.  Feature Segmentation: looking only at data while grasping objects  \n",
      "3.  Push to HMM training   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Globals\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Client & View:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "from datetime import datetime\n",
      "import scipy.cluster.vq as vq\n",
      "import vector_quantization_refactor as vqr\n",
      "import segmentation_refactor as segr\n",
      "import data_wrangling_page as dwp\n",
      "import cStringIO\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "def getClient():\n",
      "    ipclient = Client('/home/ubuntu/.starcluster/ipcluster/simcluster-us-east-1.json'\n",
      "                      ,sshkey='/home/ubuntu/.ssh/simcluster.rsa'\n",
      "                      ,packer='pickle')\n",
      "    ipview = ipclient[:]\n",
      "    return ipview, ipclient\n",
      "\n",
      "ipview, ipclient = getClient()\n",
      "print ipview\n",
      "print ipclient\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<DirectView [0, 1, 2, 3,...]>\n",
        "<IPython.parallel.client.client.Client object at 0x38eb350>\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Raw Features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getFeatures(table_name, task, dataset=None, sensors=None):\n",
      "    sensors = sensors if sensors else getSensors(table_name)\n",
      "    dataset = dataset if dataset else table_name\n",
      "    thresholds = {task: vqr.getThresholds(table_name, dataset, task)}\n",
      "    outliers = vqr.qOutliers(task, table_name, thresholds, sensors)\n",
      "    results = dwp.queryGoogle(outliers)\n",
      "    return results, thresholds\n",
      "\n",
      "    #return (np.array([[vqr.floats(field['v']) for field in row['f']]for row in results['rows']]), thresholds[task])\n",
      "dataset = 'timdataMatlab'\n",
      "table_name = 'timdata'\n",
      "task = 'suturing'\n",
      "tasks = ['cutting', 'suturing', 'pegtransfer']\n",
      "sensors = ['QgL', 'FgL','QgR', 'FgR'] \n",
      "\n",
      "import numpy as np\n",
      "raw, thresholds = getFeatures(table_name, task, dataset, sensors)\n",
      "test_features = np.array([[vqr.floats(field['v']) for field in row['f']]for row in raw['rows']])\n",
      "print test_features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current length:  456\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "returning data\n",
        "current length: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25447\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  50844\n",
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  76277\n",
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  101714\n",
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  127177\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  152631\n",
        "6 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  178057\n",
        "7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  203489\n",
        "8 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  228877\n",
        "9 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  254275\n",
        "10 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  279664\n",
        "11 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  305069\n",
        "12 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  330514\n",
        "13 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  355988\n",
        "14 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  381450\n",
        "15 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  406838\n",
        "16 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  432241\n",
        "17 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  457655\n",
        "18 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  483139\n",
        "19 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  508566\n",
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  534024\n",
        "21 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  559430\n",
        "22 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  584874\n",
        "23 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current row:  602956\n",
        "24 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "returning data\n",
        "[[ u'/home/ubuntu/simulab/data/CSVs/UH_Subj302_Suturing_02.02.2011-10.37.07_EDGE3'\n",
        "  u'-19.388' u'13.635' u'-14.254' u'9.2629']\n",
        " [ u'/home/ubuntu/simulab/data/CSVs/UH_Subj302_Suturing_02.02.2011-10.37.07_EDGE3'\n",
        "  u'-19.388' u'13.635' u'-14.254' u'9.2629']\n",
        " [ u'/home/ubuntu/simulab/data/CSVs/UH_Subj302_Suturing_02.02.2011-10.37.07_EDGE3'\n",
        "  u'-19.388' u'13.635' u'-14.254' u'9.2629']\n",
        " ..., \n",
        " [ u'/home/ubuntu/simulab/data/CSVs/UMN_Subj8_Suturing_12.10.2010-07.38.51_EDGE3'\n",
        "  u'9.653' u'0.781' u'-14.104' u'10.589']\n",
        " [ u'/home/ubuntu/simulab/data/CSVs/UMN_Subj8_Suturing_12.10.2010-07.38.51_EDGE3'\n",
        "  u'9.941' u'0.642' u'-14.104' u'10.102']\n",
        " [ u'/home/ubuntu/simulab/data/CSVs/UMN_Subj8_Suturing_12.10.2010-07.38.51_EDGE3'\n",
        "  u'10.229' u'0.642' u'-14.104' u'10.241']]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Vector Quantization\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#features, thresholds = getFeatures('timdata', task, sensors)\n",
      "def create_codebook(args):\n",
      "    features, size = args\n",
      "    \n",
      "    import scipy.cluster as c\n",
      "    \n",
      "    left = c.vq.kmeans(features[0], size, 1) #if features[0] else None\n",
      "    right = c.vq.kmeans(features[1], size, 1) #if features[1] else None\n",
      "        \n",
      "    return (left, right)\n",
      "\n",
      "'''\n",
      "#Normalize\n",
      "print 'normalizing'\n",
      "normFeatures = vqr.normalize(features, thresholds, sensors)\n",
      "leftFeat = normFeatures[:,:2]\n",
      "rightFeat = normFeatures[:,2:]\n",
      "\n",
      "print size,\n",
      "size = vqr.size_codebook(task)\n",
      "print ' size: ', size\n",
      "\n",
      "print 'starting timing'\n",
      "start = datetime.now()\n",
      "print start\n",
      "\n",
      "run_view = ipview.map_async(create_codebook, [((leftFeat, rightFeat), size) for i in range(10)])\n",
      "results = run_view.get()\n",
      "\n",
      "end = datetime.now()\n",
      "print end\n",
      "print end - start \n",
      "'''\n",
      "#resultsR = ipview.map_async(create_codebook, [(featuresR, size, i) for i in range(100)])\n",
      "#lowest_distortion_left = min(results[0], key=itemgetter(1))\n",
      "#lowest_distortion_right = min(results[1], key=itemgetter(1))\n",
      "#left_name = 'left_tim_codebook_{0}_QgFg_thresh'.format(task)\n",
      "#right_name = 'right_tim_codebook_{0}_QgFg_thresh'.format(task)\n",
      "\n",
      "#json.dump(lowest_distortion_left[0].tolist(),  open(left_name, 'w'))\n",
      "#json.dump(lowest_distortion_right[0].tolist(),  open(right_name, 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "normalizing\n",
        "normalizing QgL...\n",
        "normalizing FgL...\n",
        "normalizing QgR...\n",
        "normalizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " FgR...\n",
        "Done\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  size:  67\n",
        "starting timing\n",
        "2012-09-19 12:45:33.452701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2012-09-19 12:49:20.656743"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0:03:47.204042\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "normalizing\n",
        "normalizing QgL...\n",
        "normalizing FgL...\n",
        "normalizing QgR...\n",
        "normalizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " FgR...\n",
        "Done\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  size:  67\n",
        "starting timing\n",
        "2012-09-19 12:49:21.108529\n",
        "2012-09-19 12:52:36.789173"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0:03:15.680644\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def upload_codebook(left, right):\n",
      "    data = cStringIO.StringIO()\n",
      "    fields = dwp.getSchemaFields('codebooks')  \n",
      "    \n",
      "    data.write('{0},{1},\"{2}\"\\n'.format(dataset + '_left', '19-SEPT-2012 1:33', json.dumps(left[0].tolist())))   \n",
      "    data.write('{0},{1},\"{2}\"\\n'.format(dataset + '_right', '19-SEPT-2012 1:33', json.dumps(right[0].tolist())))\n",
      "\n",
      "    body = dwp.getBody(data.getvalue(), fields, 'codebooks', 'data'\n",
      "                              , createDisposition='CREATE_IF_NEEDED'\n",
      "                              , writeDisposition='WRITE_APPEND')\n",
      "    dwp.loadTableFromCSV(body)\n",
      "    \n",
      "\n",
      "\n",
      "#upload_codebook(min(results[0], key=itemgetter(1)), min(results[1], key=itemgetter(1)))\n",
      "#print tst.getvalue()\n",
      "        \n",
      "#dataset = 'timdataMatlab'\n",
      "#table_name = 'timdata'\n",
      "\n",
      "#tasks = ['cutting', 'suturing', 'pegtransfer']\n",
      "#sensors = ['QgL', 'FgL','QgR', 'FgR'] \n",
      "\n",
      "#for task in task:\n",
      "\n",
      "#features, thresholds = getFeatures(table_name, 'suturing', dataset, sensors)\n",
      "#rv = upload_codebook('timdataMatlab', features, thresholds, task, sensors)\n",
      "\n",
      "#print features[0]\n",
      "#print thresholds\n",
      "#print task\n",
      "\n",
      "#size = vqr.size_codebook(task)\n",
      "#print size\n",
      "#print run_view.ready()\n",
      "\n",
      "\n",
      "#x= run_view.get()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''from operator import itemgetter\n",
      "import sys\n",
      "print tst\n",
      "s = tst.getvalue()\n",
      "print sys.getsizeof(s)\n",
      "print s'''\n",
      "#distortions = [d[1] for d in results[0]]\n",
      "#print len(distortions)\n",
      "#mini= min(results[0], key=itemgetter(1))\n",
      "#print mini[1]\n",
      "#print distortions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "'from operator import itemgetter\\nimport sys\\nprint tst\\ns = tst.getvalue()\\nprint sys.getsizeof(s)\\nprint s'"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Segmentation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def segment(args):\n",
      "    import numpy as np\n",
      "    task, thresholds, kv = args\n",
      "    task_threshold = thresholds.get(task, (0, 0))\n",
      "    \n",
      "    key = kv[0] \n",
      "    arr = np.array(kv[1])\n",
      "    arr = arr.astype(float) #Convert from unicode to float\n",
      "    \n",
      "    lidx, = np.diff(arr[:, 0] > task_threshold[0]).nonzero()\n",
      "    lidx = np.delete(lidx,-1) if len(lidx)%2==1 else lidx\n",
      "    lidx = np.append(np.matrix(lidx[range(0,len(lidx),2)]).transpose(), np.matrix(lidx[range(1,len(lidx),2)]).transpose() , 1)\n",
      "    lidx = np.squeeze(np.asarray(lidx)) #lidx = lidx.shape(-1,2) #puts idx into a two column matrix\n",
      "\n",
      "    ridx, = np.diff(arr[:, 1] > task_threshold[0]).nonzero()\n",
      "    ridx = np.delete(ridx,-1) if len(ridx)%2==1 else ridx\n",
      "        \n",
      "    ridx = np.append(np.matrix(ridx[range(0,len(ridx),2)]).transpose(), np.matrix(ridx[range(1,len(ridx),2)]).transpose() , 1)\n",
      "    ridx = np.squeeze(np.asarray(ridx)) #ridx = ridx.shape(-1,2) #puts idx into a two column matrix\n",
      "\n",
      "    #Check to make sure grasp is longer than time thresh:: IndexError: invalid index\n",
      "    e = None\n",
      "    try:\n",
      "        left =  lidx[lidx[:, 1] - lidx[:, 0] > task_threshold[1]]\n",
      "    except Exception as e:\n",
      "        left = None\n",
      "    \n",
      "    try:\n",
      "        right = ridx[ridx[:, 1] - ridx[:, 0] > task_threshold[1]]\n",
      "    except Exception as e:\n",
      "        right = None \n",
      "    \n",
      "    return (key, left, right, e)\n",
      "\n",
      "def run_segmentation(task, thresholds, data):\n",
      "    start = datetime.now()\n",
      "    print task, ': \\n', start\n",
      "    \n",
      "    run_view = ipview.map_async(segment, [(task, thresholds, kv) for kv in data.iteritems()])\n",
      "    results = run_view.get()\n",
      "    \n",
      "    end = datetime.now()\n",
      "    print end\n",
      "    print end - start\n",
      "    return results\n",
      "    \n",
      "#put this in a function & loop\n",
      "#function checks if segments exist\n",
      "#if not it checks google\n",
      "#if not there it kicks off the creation\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grasp_thresholds = {'suturing': (1.5, 0.1*30), 'cutting': (3, 0.5*30), 'pegtransfer': (3, 1*30)} \n",
      "#ipview, ipclient = getClient()\n",
      "\n",
      "try:\n",
      "    FgLR\n",
      "except NameError:\n",
      "    FgLR = None\n",
      "\n",
      "if FgLR is None:\n",
      "    print 'Loading data...'\n",
      "    FgLR = segr.getData(table_name, task)\n",
      "    print 'data got...'\n",
      "    \n",
      "#Segment FgL and R based on force and time thresholds\n",
      "seg_results = run_segmentation(task, grasp_thresholds, FgLR)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2012-09-21 00:49:49.490195\n",
        "2012-09-21 00:50:12.136149"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0:00:22.645954\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 221,
       "text": [
        "\"\\nstart = datetime.now()\\nprint task, ': \\n', start\\nrun_view = ipview.map_async(segment, [(task, grasp_thresholds, kv) for kv in FgLR.iteritems()])\\nseg_results = run_view.get()\\n\\nend = datetime.now()\\nprint end\\nprint end - start\\n\""
       ]
      }
     ],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split(arr, cond):\n",
      "  return [arr[cond], arr[~cond]]\n",
      "\n",
      "\n",
      "def splitFeatures(rest):\n",
      "    features_split = {}\n",
      "    \n",
      "    while (rest[-1,0]!=rest[0,0]):   #.any()\n",
      "        cut, rest = split (rest, rest[:,0]==rest[0,0])\n",
      "        features_split[cut[0,0]] = cut[:,1:]\n",
      "                     \n",
      "    features_split[rest[0,0]] = rest[:,1:]\n",
      "    return features_split \n",
      "\n",
      "testfiles = splitFeatures(test_features)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for item in seg_results:\n",
      "    \n",
      "from collections import defaultdict\n",
      "\n",
      "\n",
      "def getGrasps(seg_results, testfiles):\n",
      "    graspsegs = defaultdict(list) \n",
      "    \n",
      "    for item in seg_results:\n",
      "        #sensors = ['QgL', 'FgL','QgR', 'FgR'] \n",
      "        key = item[0]\n",
      "        #print key\n",
      "        xx = testfiles[item[0]]\n",
      "        xx = xx.astype(float)\n",
      "        graspsegs[key] = []\n",
      "        try: #Get grasps for left side, checking there are left indices\n",
      "            graspsegs[key] = []\n",
      "            for i in range(len(item[1])):   \n",
      "                #graspsegs[key]['left'][i] = xx[:,(0,2)][item[1][i,0] : item[1][i,1]] \n",
      "                graspsegs[key].append(['left', i, xx[:,(0,1)][item[1][i,0] : item[1][i,1]]] )\n",
      "        except Exception as e:\n",
      "            pass\n",
      "            #graspsegs[key].append(['left', i, None ])\n",
      "        \n",
      "        try: #Get grasps for right side, checking there are left indices\n",
      "            for i in range(len(item[2])):\n",
      "                #graspsegs[key]['right'][i] = xx[:,(1,3)][item[2][i,0] : item[2][i,1]]\n",
      "                graspsegs[key].append(['right', i, xx[:,(2,3)][item[1][i,0] : item[1][i,1]]] )\n",
      "        except Exception as e:\n",
      "            pass\n",
      "            #graspsegs[key]['right'] = None\n",
      "            #graspsegs[key].append(['right', i, None ])\n",
      "        \n",
      "    return graspsegs\n",
      "        \n",
      "graspsegs = getGrasps(seg_results, testfiles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 185
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print graspsegs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 186
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skills = open('/home/ubuntu/simulab/data/contentExpertLevel.csv')\n",
      "\n",
      "skilldict = {'/home/ubuntu/simulab/data/CSVs/'+line.split(',')[0].strip(): line.split(',')[2].strip() for line in skills if line.strip()} \n",
      "#print skilldict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def upload_getGrasps(graspsegs, skilldict):    \n",
      "    data = cStringIO.StringIO()\n",
      "    fields = dwp.getSchemaFields('segments')  \n",
      "\n",
      "    for key, vlist in graspsegs.iteritems():\n",
      "        skill = skilldict[key.strip()+'.txt']\n",
      "        for v in vlist:\n",
      "            if v:\n",
      "                data.write( '{0},{1},{2},{3},{4},\"{5}\",\"{6}\"\\n'.format(key, 'timdata', skill, v[0], v[1], json.dumps(v[2][:,0].tolist()), json.dumps(v[2][:,1].tolist())) )\n",
      "           # else:\n",
      "                #print key, skill, v[0], v[1], json.dumps(v[2].tolist())\n",
      "                \n",
      "    #body = dwp.getBody(data.getvalue(), fields, 'segments', 'data'\n",
      "    #                         , createDisposition='CREATE_IF_NEEDED'\n",
      "    #                          , writeDisposition='WRITE_APPEND')\n",
      "    #dwp.loadTableFromCSV(body)\n",
      "    return data\n",
      "data = upload_getGrasps(graspsegs, skilldict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current length:  7\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "returning data\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def codebookApply(features, codebook):\n",
      "    '''\n",
      "    returns the code and distance for the codebook\n",
      "    '''\n",
      "    return vq.vq(features, codebook)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "'''\n",
      "codebooks = dwp.queryGoogle('SELECT dataset,json FROM data.codebooks')\n",
      "\n",
      "cbleft= codebooks['rows'][0]['f'][1]['v']\n",
      "cbright= codebooks['rows'][1]['f'][1]['v']\n",
      "\n",
      "cbl = np.array(json.loads(cbleft),dtype = np.float)\n",
      "cbr = np.array(json.loads(cbright),dtype = np.float)\n",
      "'''\n",
      "def encodeSegSkillData(graspsegs,skilldict,cbl,cbr):\n",
      "    encodedExpL = []#defaultdict(list) \n",
      "    encodedNovL = []#defaultdict(list) \n",
      "    encodedExpR = []#defaultdict(list) \n",
      "    encodedNovR = []#defaultdict(list) \n",
      "    for key, vlist in graspsegs.iteritems():\n",
      "        skill = skilldict[key.strip()+'.txt']\n",
      "        \n",
      "        for v in vlist:\n",
      "            if v:\n",
      "                hand= v[0]\n",
      "                if skill == 'Expert' and hand == 'left':\n",
      "                    code,dist = vq.vq(v[2], cbl) \n",
      "                    encodedExpL.append(code) \n",
      "                elif skill == 'Novice' and hand == 'left':\n",
      "                    code, dist= vq.vq(v[2], cbl) \n",
      "                    encodedNovL.append(code)\n",
      "                elif skill == 'Expert' and hand == 'right':\n",
      "                    code,dist= vq.vq(v[2], cbr) \n",
      "                    encodedExpR.append(code)\n",
      "                elif skill == 'Novice' and hand == 'right':\n",
      "                    code,dist = vq.vq(v[2], cbr) \n",
      "                    encodedNovR.append(code )\n",
      "                        \n",
      "            #end of v in vlist\n",
      "        #end of key, vlist in graspsegs\n",
      "        return encodedExpL, encodedNovL, encodedExpR, encodedNovL\n",
      "\n",
      "encodedExpL, encodedNovL, encodedExpR, encodedNovR = encodeSegSkillData(graspsegs,skilldict,cbl,cbr)\n",
      "\n",
      "\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 348
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = \"\"\"\"Your humble writer knows a little bit about a lot of things, but despite writing a fair amount about text processing (a book, for example), linguistic processing is a relatively novel area for me. Forgive me if I stumble through my explanations of the quite remarkable Natural Language Toolkit (NLTK), a wonderful tool for teaching, and working in, computational linguistics using Python. Computational linguistics, moreover, is closely related to the fields of artificial intelligence, language/speech recognition, translation, and grammar checking.\\nWhat NLTK includes\\nIt is natural to think of NLTK as a stacked series of layers that build on each other. Readers familiar with lexing and parsing of artificial languages (like, say, Python) will not have too much of a leap to understand the similar -- but deeper -- layers involved in natural language modeling.\\nGlossary of terms\\nCorpora: Collections of related texts. For example, the works of Shakespeare might, collectively, by called a corpus; the works of several authors, corpora.\\nHistogram: The statistic distribution of the frequency of different words, letters, or other items within a data set.\\nSyntagmatic: The study of syntagma; namely, the statistical relations in the contiguous occurrence of letters, words, or phrases in corpora.\\nContext-free grammar: Type-2 in Noam Chomsky's hierarchy of the four types of formal grammars. See Resources for a thorough description.\\nWhile NLTK comes with a number of corpora that have been pre-processed (often manually) to various degrees, conceptually each layer relies on the processing in the adjacent lower layer. Tokenization comes first; then words are tagged; then groups of words are parsed into grammatical elements, like noun phrases or sentences (according to one of several techniques, each with advantages and drawbacks); and finally sentences or other grammatical units can be classified. Along the way, NLTK gives you the ability to generate statistics about occurrences of various elements, and draw graphs that represent either the processing itself, or statistical aggregates in results.\\nIn this article, you'll see some relatively fleshed-out examples from the lower-level capabilities, but most of the higher-level capabilities will be simply described abstractly. Let's now take the first steps past text processing, narrowly construed. \"\"\"\n",
      "sentences = s.split('.')[:-1]\n",
      "seq = [map(lambda x:(x,''), ss.split(' ')) for ss in sentences]\n",
      "symbols = list(set([ss[0] for sss in seq for ss in sss]))\n",
      "states = range(5)\n",
      "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
      "m = trainer.train_unsupervised(seq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0 logprob -2843.42261771\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 logprob -2661.17748749\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 logprob -2661.17748749\n"
       ]
      }
     ],
     "prompt_number": 311
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "formattedNovR=[]\n",
      "for seg in encodedNovR:\n",
      "    segtuple = []\n",
      "    for s in seg:\n",
      "        segtuple.append((int(s),''))\n",
      "    \n",
      "    formattedNovR.append(segtuple)\n",
      "    \n",
      "import nltk \n",
      "nltkTrainer = nltk.tag.hmm.HiddenMarkovModelTrainer(range(15),range(cbr.shape[0]))\n",
      "trained = nltkTrainer.train_unsupervised(formattedNovR, max_iterations=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0 logprob -46284.2605232\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 logprob -6181.45611282\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 logprob -6181.45611282\n"
       ]
      }
     ],
     "prompt_number": 415
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print trained"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<HiddenMarkovModelTagger 15 states and 67 output symbols>\n"
       ]
      }
     ],
     "prompt_number": 416
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a,b = formattedNovR[1][0]\n",
      "print a\n",
      "print type(int(a))\n",
      "\n",
      "print type(range(cbr.shape[0])[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "58\n",
        "<type 'int'>\n",
        "<type 'int'>\n"
       ]
      }
     ],
     "prompt_number": 388
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = len(range(cbr.shape[0])) \n",
      "symbol_dict = dict((range(cbr.shape[0])[i], i) for i in range(M)) \n",
      "\n",
      "for sequence in formattedNovR:\n",
      "    sequence = list(sequence)\n",
      "    T = len(sequence)\n",
      "    for t in range(T):\n",
      "        x = sequence[t][0]\n",
      "    \n",
      "        #a,b = formattedNovR[1][0]\n",
      "        xi = symbol_dict[x]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 413
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print seq[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('\"Your', '')\n"
       ]
      }
     ],
     "prompt_number": 407
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}